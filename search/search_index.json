{"config":{"lang":["en","fr","es","de","ar"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to db2ixf","text":"<p>DB2IXF parser is an open-source python package that simplifies the parsing and processing of IXF files.</p> <p>Source code: Source</p> <p>Raise an issue: Tracker</p>"},{"location":"markdown/getting-started/","title":"Getting Started","text":"<p>To begin using the IXF Parser package, follow the installation instructions below.</p>"},{"location":"markdown/getting-started/#installation","title":"Installation","text":"<p>Ensure that you have set up and activated a Python virtual environment. Then, use the following command to install the package:</p> Bash<pre><code>pip install db2ixf\n</code></pre>"},{"location":"markdown/getting-started/#examples","title":"Examples","text":"<p>Below are examples demonstrating how to use the IXF Parser package:</p>"},{"location":"markdown/getting-started/#parsing-an-ixf-file","title":"Parsing an IXF File","text":"<p>You can parse an IXF file by providing a file-like object or a path to the file. Here\u2019s an example using a file-like object:</p> Python<pre><code># coding=utf-8\nfrom pathlib import Path\nfrom db2ixf import IXFParser\n\npath = Path('path/to/IXF/file.XXX.IXF')\nwith open(path, mode='rb') as f:\n    parser = IXFParser(f)\n    # rows = parser.parse()  # Deprecated !\n    rows = parser.get_row()  # Python generator\n    for row in rows:\n        print(row)\n\nwith open(path, mode='rb') as f:\n    parser = IXFParser(f)\n    rows = parser.get_all_rows()  # Loads into memory !\n    for row in rows:\n        print(row)\n</code></pre> <p>In this example, the <code>IXFParser</code> is initialized with a file-like object <code>f</code>, and the <code>get_row</code> method is used to retrieve the parsed rows as a list of dictionaries. <code>get_row</code> is a python generator, it helps when you deal with big files. <code>get_all_rows</code> will load all rows into memory as a python list so use it in case you have small files.</p>"},{"location":"markdown/getting-started/#converting-to-json","title":"Converting to JSON","text":"<p>You can convert the parsed data to JSON format and save it to a file. Here\u2019s an example:</p> Python<pre><code># coding=utf-8\nfrom pathlib import Path\nfrom db2ixf.ixf import IXFParser\n\npath = Path('Path/to/IXF/FILE/XXX.IXF')\nwith open(path, mode='rb') as f:\n    parser = IXFParser(f)\n    output_path = Path('Path/To/Output/YYY.json')\n    with open(output_path, mode='w', encoding='utf-8') as output_file:\n        parser.to_json(output_file)\n</code></pre> <p>In this example, the parsed data is converted to JSON format using the <code>to_json</code> method and saved to the specified output file.</p>"},{"location":"markdown/getting-started/#converting-to-jsonline","title":"Converting to JSONLINE","text":"<p>You can convert the parsed data to JSONLINE format and save it to a file. Here\u2019s an example:</p> Python<pre><code># coding=utf-8\nfrom pathlib import Path\nfrom db2ixf.ixf import IXFParser\n\npath = Path('Path/to/IXF/FILE/XXX.IXF')\nwith open(path, mode='rb') as f:\n    parser = IXFParser(f)\n    output_path = Path('Path/To/Output/YYY.jsonl')\n    with open(output_path, mode='w', encoding='utf-8') as output_file:\n        parser.to_jsonline(output_file)\n</code></pre> <p>In this example, the parsed data is converted to JSONLINE format using the <code>to_jsonline</code> method and saved to the specified output file.</p>"},{"location":"markdown/getting-started/#converting-to-csv","title":"Converting to CSV","text":"<p>You can also convert the parsed data to CSV format and save it to a file. Here\u2019s an example:</p> Python<pre><code># coding=utf-8\nimport pathlib\nfrom db2ixf.ixf import IXFParser\n\npath = pathlib.Path('Path/to/IXF/FILE/XXX.IXF')\nwith open(path, mode='rb') as f:\n    parser = IXFParser(f)\n    output_path = pathlib.Path('Path/To/Output/YYY.csv')\n    with open(output_path, mode='w', encoding='utf-8') as output_file:\n        parser.to_csv(output_file, sep='#')\n</code></pre> <p>In this example, the parsed data is converted to CSV format using the <code>to_csv</code> method and saved to the specified output file. The <code>sep</code> parameter specifies the separator/delimiter to be used in the CSV file.</p>"},{"location":"markdown/getting-started/#converting-to-parquet","title":"Converting to Parquet","text":"<p>If you prefer to store the parsed data in Parquet format, you can use the following example:</p> Python<pre><code># coding=utf-8\nfrom pathlib import Path\nfrom db2ixf.ixf import IXFParser\n\npath = Path('Path/to/IXF/FILE/XXX.IXF')\nwith open(path, mode='rb') as f:\n    parser = IXFParser(f)\n    output_path = Path('Path/To/Output/YYY.parquet')\n    with open(output_path, mode='wb') as output_file:\n        parser.to_parquet(output_file)\n</code></pre> <p>In this example, the parsed data is converted to Parquet format using the <code>to_parquet</code> method and saved to the specified output file.</p>"},{"location":"markdown/getting-started/#converting-to-deltalake","title":"Converting to Deltalake","text":"<p>If you prefer to store the parsed data in Deltalake format, you can use the following example:</p> Python<pre><code># coding=utf-8\nfrom pathlib import Path\nfrom db2ixf.ixf import IXFParser\n\npath = Path('Path/to/IXF/FILE/XXX.IXF')\nwith open(path, mode='rb') as f:\n    parser = IXFParser(f)\n    output_path = Path('Path/To/Output/Table')\n    parser.to_deltalake(output_path)\n</code></pre> <p>In this example, the parsed data is converted to Deltalake format using the <code>to_deltalake</code> method and saved to the specified output path.</p> <p>You can also use a string but Path is better in case you work on a local filesystem. When we use a string, it is often for a remote storage and in this case you can either use filesystem argument or let <code>deltalake</code> package infer it from the uri.</p> <p>The IXF Parser package provides flexibility in terms of input and output options, allowing you to easily parse and process IXF files according to your needs.</p>"},{"location":"markdown/getting-started/#precautions","title":"Precautions","text":"<p>There are cases where the parsing can fail and sometimes can lead to data loss:</p> <ol> <li>Completely corrupted ixf file: It is usually an extraction issue.</li> <li>Partially corrupted ixf file, it contains some corrupted Rows/Lines that the    parser can not parse.<ol> <li>Parser calculates rate of corrupted rows then compares it to an accepted    rate of corrupted rows which you can set by this environment variable    <code>DB2IXF_ACCEPTED_CORRUPTION_RATE</code>(int = 1)%.</li> <li>If the rate of corrupted rows is bigger than the accepted rate the parser    raises an exception.</li> </ol> </li> <li>Unsupported data type : please contact the owners/maintainers/contributors so    you can get help otherwise any PR is welcomed.</li> </ol> 4. case: encoding issues <p>Parsing can lead to data loss in case the found or the detected encoding is  not able to decode some extracted fields/columns. </p> <p>Parser tries to decode using:</p> Text Only<pre><code>1. The found encoding (found in the column record)\n\n2. Other encodings like cp437\n\n3. The detected encoding using a third party package (chardet)\n\n4. Encodings like utf-8 and utf-32\n\n5. Ignore errors which can lead to data loss !\n</code></pre> <p>Before use the package in production, try to test in debug mode so you can detect data loss.</p>"},{"location":"markdown/getting-started/#cli","title":"CLI","text":"<p>Start with this:</p> Bash Command<pre><code>db2ixf --help\n</code></pre> Command Result<pre><code> Usage: db2ixf [OPTIONS] COMMAND [ARGS]...\n\n A command-line tool (CLI) for parsing and converting IXF (IBM DB2 \n Import/Export Format) files to various formats such as JSON, JSONLINE, CSV and \n Parquet. Easily parse and convert IXF files to meet your data processing needs.\n\n+- Options -------------------------------------------------------------------+\n| --version             -v        Show the version of the CLI.                |\n| --install-completion            Install completion for the current shell.   |\n| --show-completion               Show completion for the current shell, to   |\n|                                 copy it or customize the installation.      |\n| --help                          Show this message and exit.                 |\n+-----------------------------------------------------------------------------+\n+- Commands ------------------------------------------------------------------+\n| csv      Parse ixf FILE and convert it to a csv OUTPUT.                     |\n| json     Parse ixf FILE and convert it to a json OUTPUT.                    |\n| jsonline     Parse ixf FILE and convert it to a jsonline OUTPUT.            |\n| parquet  Parse ixf FILE and convert it to a parquet OUTPUT.                 |\n+-----------------------------------------------------------------------------+\n\n Made with heart :D\n</code></pre> <p>The <code>db2ixf</code> command-line tool (CLI) is used for parsing and converting IXF (IBM DB2 Import/Export Format) files to various formats such as JSON, JSONLINE, CSV and Parquet. It provides an easy way to parse and convert IXF files to meet your data processing needs.</p> <p>Options:</p> <ul> <li><code>--version</code> or <code>-v</code>: Show the version of the CLI.</li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or   customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>csv</code>: Parse the specified <code>ixf</code> FILE and convert it to a CSV OUTPUT.</li> <li><code>json</code>: Parse the specified <code>ixf</code> FILE and convert it to a JSON OUTPUT.</li> <li><code>jsonline</code>: Parse the specified <code>ixf</code> FILE and convert it to a JSONLINE   OUTPUT.</li> <li><code>parquet</code>: Parse the specified <code>ixf</code> FILE and convert it to a Parquet OUTPUT.</li> </ul> <p>This CLI tool is made with love ! \u2764\ufe0f</p>"},{"location":"markdown/getting-started/#examples_1","title":"Examples","text":"<p>There are 4 commands and each one is related to an output format. <code>db2ixf</code> supports only <code>json</code>, <code>jsonline</code>, <code>csv</code> and <code>parquet</code>.</p> jsonjsonlinecsvparquet Bash<pre><code>db2ixf json \"Path/to/IXF/file.IXF\"\n</code></pre> Bash<pre><code>db2ixf jsonline \"Path/to/IXF/file.IXF\"\n</code></pre> Bash<pre><code>db2ixf csv \"Path/to/IXF/file.IXF\"\n</code></pre> Bash<pre><code>db2ixf parquet \"Path/to/IXF/file.IXF\"\n</code></pre> <p>Note</p> <p>In the example above, the output file will be created in directory where you launch the command. The name of output file will be the same as the ixf  file.</p> <p>These are complete examples for all the commands:</p> jsonjsonlinecsvparquet Bash<pre><code>db2ixf json -vvv \"Path/to/IXF/file.IXF\" \"Path/to/OUTPUT/file.json\"\n</code></pre> Bash<pre><code>db2ixf jsonline -vvv \"Path/to/IXF/file.IXF\" \"Path/to/OUTPUT/file.jsonl\"\n</code></pre> Bash<pre><code>db2ixf csv -vvv --sep \"!\" \"Path/to/IXF/file.IXF\" \"Path/to/OUTPUT/file.csv\"\n</code></pre> Bash<pre><code>db2ixf parquet -vvv --version \"1.0\" --batch-size 4000 \"Path/to/IXF/file.IXF\" \"Path/to/OUTPUT/file.parquet\"\n</code></pre> <p>Tip</p> <p>Before using one of the examples, please, try <code>db2ixf &lt;command&gt; --help</code> to get details on how to use the command.</p> <p>Info</p> <p>CLI does not support the deltalake format. In case, you need support please create an issue in Github.</p>"},{"location":"markdown/introduction/","title":"Story","text":""},{"location":"markdown/introduction/#definition","title":"Definition","text":"<p>IXF stands for \u201cIBM eXchange Format,\u201d and it is a file format used by IBM\u2019s DB2 database system for data import and export operations. The IXF format provides a standardized and efficient way to exchange data between DB2 databases or between DB2 and other systems.</p>"},{"location":"markdown/introduction/#context","title":"Context","text":"<p>At work, we encountered the need to extract data from our DB2 database for analysis purposes. However, obtaining the data in a usable format proved to be a challenge. The IT department provided us with IXF files containing the exported data, but we required a solution to parse and process this data effectively.</p>"},{"location":"markdown/introduction/#solution","title":"Solution","text":"<p>To address this issue, I developed a package that simplifies the parsing of IXF files. This package builds upon existing open-source projects, which proved to be valuable resources. By leveraging these projects, I was able to create a package that streamlines the parsing of IXF files and offers various output options.</p>"},{"location":"markdown/introduction/#package-features","title":"Package Features","text":"<p>The IXF Parser package offers the following features:</p> <ol> <li> <p>Parsing IXF Files: The package allows for the parsing of IXF files,    extracting the rows of data contained within.</p> </li> <li> <p>Conversion to Multiple Formats: The parsed data can be converted to    different formats, including JSON, JSONLINE, CSV, Parquet and    Deltalake.</p> </li> <li> <p>Support for File-Like Objects: The package supports file-like objects as    input, enabling the direct parsing of IXF data from file objects.</p> </li> <li> <p>Minimal dependencies: The package has few dependencies (ebcdic, pyarrow,    deltalake, chardet, typer) which are automatically installed alongside the    package.</p> </li> <li> <p>CLI: Command line tool called <code>db2ixf</code> (Does not support Deltalake).</p> </li> </ol>"},{"location":"markdown/welcome/","title":"Welcome to db2ixf","text":"<p>DB2IXF parser is an open-source python package that simplifies the parsing and processing of IXF files.</p> <p>Source code: Source</p> <p>Raise an issue: Tracker</p>"},{"location":"markdown/code/collectors/","title":"Collectors","text":""},{"location":"markdown/code/collectors/#db2ixf.collectors","title":"<code>collectors</code>","text":"<p>Collects data from the fields extracted from the data records (D).</p>"},{"location":"markdown/code/collectors/#db2ixf.collectors-classes","title":"Classes","text":""},{"location":"markdown/code/collectors/#db2ixf.collectors-functions","title":"Functions","text":""},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_binary","title":"<code>collect_binary(c, fields, pos)</code>","text":"<p>Collects BINARY data type from ixf as a string.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Binary string.</p> <p>Raises:</p> Type Description <code>DataCollectorError</code> <p>When length exceeds 254 bytes.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_binary(c, fields, pos) -&gt; str:\n    \"\"\"Collects BINARY data type from ixf as a string.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    str\n        Binary string.\n\n    Raises\n    ------\n    DataCollectorError\n        When length exceeds 254 bytes.\n    \"\"\"\n    length = int(c[\"IXFCLENG\"])\n\n    if length &gt; 254:\n        msg = \"Length of a binary data types should not exceed 254 bytes.\"\n        raise DataCollectorError(msg)\n\n    field = fields[pos:pos + length]\n\n    return field\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_smallint","title":"<code>collect_smallint(c, fields, pos)</code>","text":"<p>Collects SMALLINT data type from ixf as an integer.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Integer.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_smallint(c, fields, pos) -&gt; int:  # noqa\n    \"\"\"Collects SMALLINT data type from ixf as an integer.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    int\n        Integer.\n    \"\"\"\n    field = int(unpack(\"&lt;h\", fields[pos:pos + 2])[0])\n\n    return field\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_integer","title":"<code>collect_integer(c, fields, pos)</code>","text":"<p>Collects INTEGER data type from ixf as an integer.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Integer.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_integer(c, fields, pos) -&gt; int:  # noqa\n    \"\"\"Collects INTEGER data type from ixf as an integer.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    int\n        Integer.\n    \"\"\"\n    field = int(unpack(\"&lt;i\", fields[pos:pos + 4])[0])\n\n    return field\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_bigint","title":"<code>collect_bigint(c, fields, pos)</code>","text":"<p>Collects BIGINT data type from ixf as an integer.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Integer.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_bigint(c, fields, pos) -&gt; int:  # noqa\n    \"\"\"Collects BIGINT data type from ixf as an integer.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    int\n        Integer.\n    \"\"\"\n    field = int(unpack(\"&lt;q\", fields[pos:pos + 8])[0])\n\n    return field\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_decimal","title":"<code>collect_decimal(c, fields, pos)</code>","text":"<p>Collects DECIMAL data type from ixf as a integer or a float.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>Union[int, float]</code> <p>Integer or Float.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_decimal(c, fields, pos) -&gt; Union[int, float]:\n    \"\"\"Collects DECIMAL data type from ixf as a integer or a float.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    Union[int, float]\n        Integer or Float.\n    \"\"\"\n    p = int(c[\"IXFCLENG\"][0:3])\n    s = int(c[\"IXFCLENG\"][3:5])\n    length = int((p + 2) / 2)\n    field = fields[pos:pos + length]\n\n    dec = 0.0\n    for b in range(0, min(len(field), length) - 1):\n        dec = dec * 100 + int(field[b] &gt;&gt; 4) * 10 + int(field[b] &amp; 0x0f)\n    dec = dec * 10 + int(field[-1] &gt;&gt; 4)\n\n    if int(field[-1] &amp; 0x0f) != 12:\n        dec = -dec\n\n    if s == 0:\n        return Decimal(dec)\n\n    return Decimal(dec / pow(10, s))\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_floating_point","title":"<code>collect_floating_point(c, fields, pos)</code>","text":"<p>Collects FLOATING POINT data type from ixf as a float.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A python float object.</p> <p>Raises:</p> Type Description <code>DataCollectorError</code> <p>When facing extra bytes.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_floating_point(c, fields, pos) -&gt; float:\n    \"\"\"Collects FLOATING POINT data type from ixf as a float.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    float\n        A python float object.\n\n    Raises\n    ------\n    DataCollectorError\n        When facing extra bytes.\n    \"\"\"\n    col_length = int(c[\"IXFCLENG\"])\n\n    if col_length == 4:\n        return float(unpack(\"&gt;f\", fields[pos:pos + col_length])[0])\n\n    if col_length == 8:\n        return float(unpack(\"&gt;d\", fields[pos:pos + col_length])[0])\n\n    raise DataCollectorError(\n        f\"Expecting 4 or 8 bytes, found {col_length} bytes\"\n    )\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_char","title":"<code>collect_char(c, fields, pos)</code>","text":"<p>Collects CHAR data type from ixf as a string.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>String.</p> <p>Raises:</p> Type Description <code>DataCollectorError</code> <p>When length exceeds 254 bytes.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_char(c, fields, pos) -&gt; str:\n    \"\"\"Collects CHAR data type from ixf as a string.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    str\n        String.\n\n    Raises\n    ------\n    DataCollectorError\n        When length exceeds 254 bytes.\n    \"\"\"\n    length = int(c[\"IXFCLENG\"])\n\n    if length &gt; 254:\n        msg = \"Length of a char data types should not exceed 254 bytes.\"\n        raise DataCollectorError(msg)\n\n    sbcp, dbcp = get_ccsid_from_column(c)\n\n    field = fields[pos:pos + length]\n\n    if dbcp != 0:\n        return decode_cell(field, dbcp, \"d\").strip()\n\n    if sbcp != 0:\n        return decode_cell(field, sbcp).strip()\n\n    return str(field, \"utf-8\").strip()\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_varchar","title":"<code>collect_varchar(c, fields, pos)</code>","text":"<p>Collects VARCHAR data type from ixf as a string.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>String.</p> <p>Raises:</p> Type Description <code>DataCollectorError</code> <p>When length of var char exceeds maximum length.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_varchar(c, fields, pos) -&gt; str:\n    \"\"\"Collects VARCHAR data type from ixf as a string.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    str\n        String.\n\n    Raises\n    ------\n    DataCollectorError\n        When length of var char exceeds maximum length.\n    \"\"\"\n    max_length = int(c[\"IXFCLENG\"])\n\n    length = int(unpack(\"&lt;h\", fields[pos:pos + 2])[0])\n    if length &gt; max_length:\n        msg = f\"Length {length} exceeds the maximum length {max_length}.\"\n        raise DataCollectorError(msg)\n\n    pos += 2\n\n    sbcp, dbcp = get_ccsid_from_column(c)\n\n    field = fields[pos:pos + length]\n\n    if dbcp != 0:\n        return decode_cell(field, dbcp, \"d\").strip()\n\n    if sbcp != 0:\n        return decode_cell(field, sbcp).strip()\n\n    return str(field, \"utf-8\").strip()\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_longvarchar","title":"<code>collect_longvarchar(c, fields, pos)</code>","text":"<p>Collects LONGVARCHAR data type from ixf as a string.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>String.</p> <p>Raises:</p> Type Description <code>DataCollectorError</code> <p>When length of long var char exceeds maximum length.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_longvarchar(c, fields, pos) -&gt; str:\n    \"\"\"Collects LONGVARCHAR data type from ixf as a string.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    str\n        String.\n\n    Raises\n    ------\n    DataCollectorError\n        When length of long var char exceeds maximum length.\n    \"\"\"\n    max_length = int(c[\"IXFCLENG\"])\n    length = int(unpack(\"&lt;h\", fields[pos:pos + 2])[0])\n    if length &gt; max_length:\n        msg = f\"Length {length} exceeds the maximum length {max_length}.\"\n        raise DataCollectorError(msg)\n\n    pos += 2\n\n    sbcp, dbcp = get_ccsid_from_column(c)\n\n    field = fields[pos:pos + length]\n\n    if dbcp != 0:\n        return decode_cell(field, dbcp, \"d\").strip()\n\n    if sbcp != 0:\n        return decode_cell(field, sbcp).strip()\n\n    return str(field, \"utf-8\").strip()\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_vargraphic","title":"<code>collect_vargraphic(c, fields, pos)</code>","text":"<p>Collects VARGRAPHIC data type from ixf as a string.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>String.</p> <p>Raises:</p> Type Description <code>DataCollectorError</code> <p>When length of var graphic exceeds maximum length.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_vargraphic(c, fields, pos) -&gt; str:\n    \"\"\"Collects VARGRAPHIC data type from ixf as a string.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    str\n        String.\n\n    Raises\n    ------\n    DataCollectorError\n        When length of var graphic exceeds maximum length.\n    \"\"\"\n    max_length = int(c[\"IXFCLENG\"])\n\n    length = int(unpack(\"&lt;h\", fields[pos:pos + 2])[0])\n    if length &gt; max_length:\n        msg = f\"Length {length} exceeds the maximum length {max_length}.\"\n        raise DataCollectorError(msg)\n\n    pos += 2\n\n    _, dbcp = get_ccsid_from_column(c)\n\n    field = fields[pos:pos + (length * 2)]\n\n    if dbcp != 0:\n        return decode_cell(field, dbcp, \"d\").strip()\n\n    _msg = \"The string in double-byte characters has DBCS code page \" \\\n           \"equals to 0 (unknown encoding)\"\n    raise DataCollectorError(_msg)\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_date","title":"<code>collect_date(c, fields, pos)</code>","text":"<p>Collects DATE data type from ixf as a date object.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>date</code> <p>Date of format yyyy-mm-dd.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_date(c, fields, pos) -&gt; date:  # noqa\n    \"\"\"Collects DATE data type from ixf as a date object.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    date\n        Date of format yyyy-mm-dd.\n    \"\"\"\n    field = str(fields[pos:pos + 10], encoding=\"utf-8\").strip()\n\n    return datetime.strptime(field, \"%Y-%m-%d\").date()\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_time","title":"<code>collect_time(c, fields, pos)</code>","text":"<p>Collects TIME data type from ixf as a time object.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>time</code> <p>Time of format HH:MM:SS.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_time(c, fields, pos) -&gt; time:  # noqa\n    \"\"\"Collects TIME data type from ixf as a time object.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    time\n        Time of format HH:MM:SS.\n    \"\"\"\n    field = str(fields[pos:pos + 8], encoding=\"utf-8\").strip()\n\n    return datetime.strptime(field, \"%H.%M.%S\").time()\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_timestamp","title":"<code>collect_timestamp(c, fields, pos)</code>","text":"<p>Collects TIMESTAMP data type from ixf as a datetime object.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Timestamp of format yyyy-mm-dd-hh.mm.ss.nnnnnn or yyyy-mm-dd-hh.mm.ss.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_timestamp(c, fields, pos) -&gt; datetime:  # noqa\n    \"\"\"Collects TIMESTAMP data type from ixf as a datetime object.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    datetime\n        Timestamp of format yyyy-mm-dd-hh.mm.ss.nnnnnn or yyyy-mm-dd-hh.mm.ss.\n    \"\"\"\n    field = str(fields[pos:pos + 26], encoding=\"utf-8\").strip()\n\n    try:\n        return datetime.strptime(field, \"%Y-%m-%d-%H.%M.%S.%f\")\n    except ValueError:\n        return datetime.strptime(field, \"%Y-%m-%d-%H.%M.%S\")\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_clob","title":"<code>collect_clob(c, fields, pos)</code>","text":"<p>Collects CLOB data type from ixf as a string object.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>String representing the CLOB (Character Large Object).</p> <p>Raises:</p> Type Description <code>DataCollectorError</code> <p>When length of the large object exceeds the maximum length Or When SBCP and DBCP are simultaneously equal to 0.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_clob(c, fields, pos) -&gt; str:\n    \"\"\"Collects CLOB data type from ixf as a string object.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    str\n        String representing the CLOB (Character Large Object).\n\n    Raises\n    ------\n    DataCollectorError\n        When length of the large object exceeds the maximum length Or\n        When SBCP and DBCP are simultaneously equal to 0.\n    \"\"\"\n    max_length = int(c[\"IXFCLENG\"])\n\n    length = int(unpack(\"&lt;i\", fields[pos:pos + 4])[0])\n    if length &gt; max_length:\n        msg = f\"Length {length} exceeds the maximum length {max_length}.\"\n        raise DataCollectorError(msg)\n\n    pos += 4\n\n    sbcp, dbcp = get_ccsid_from_column(c)\n\n    field = fields[pos:pos + length]\n\n    if dbcp != 0:\n        return decode_cell(field, dbcp, \"d\").strip()\n\n    if sbcp != 0:\n        return decode_cell(field, sbcp).strip()\n\n    msg = \"CLOB data type can not be a bit string as BLOB, \" \\\n          \"the SBCP and DBCP should not simultaneously be equal to 0.\"\n    raise DataCollectorError(msg)\n</code></pre>"},{"location":"markdown/code/collectors/#db2ixf.collectors.collect_blob","title":"<code>collect_blob(c, fields, pos)</code>","text":"<p>Collects BLOB data type from ixf as a string.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>dict</code> <p>Column descriptor extracted from IXF file.</p> required <code>fields</code> <code>str</code> <p>Bytes string containing data of the row.</p> required <code>pos</code> <code>int</code> <p>Position of the column in the <code>fields</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>string representing the BLOB (Blob Large Object).</p> <p>Raises:</p> Type Description <code>DataCollectorError</code> <p>When the length of the binary large object exceeds the maximum length.</p> Source code in <code>src/db2ixf/collectors.py</code> Python<pre><code>def collect_blob(c, fields, pos) -&gt; str:\n    \"\"\"Collects BLOB data type from ixf as a string.\n\n    Parameters\n    ----------\n    c : dict\n        Column descriptor extracted from IXF file.\n    fields : str\n        Bytes string containing data of the row.\n    pos : int\n        Position of the column in the `fields`.\n\n    Returns\n    -------\n    str\n        string representing the BLOB (Blob Large Object).\n\n    Raises\n    ------\n    DataCollectorError\n        When the length of the binary large object exceeds the maximum length.\n    \"\"\"\n    max_length = int(c[\"IXFCLENG\"])\n\n    length = int(unpack(\"&lt;i\", fields[pos:pos + 4])[0])\n    if length &gt; max_length:\n        msg = f\"Length {length} exceeds the maximum length {max_length}.\"\n        raise DataCollectorError(msg)\n\n    pos += 4\n\n    sbcp, dbcp = get_ccsid_from_column(c)\n\n    field = fields[pos:pos + length]\n\n    if dbcp != 0:\n        return decode_cell(field, dbcp, \"d\").strip()\n\n    if sbcp != 0:\n        return decode_cell(field, sbcp).strip()\n\n    return field\n</code></pre>"},{"location":"markdown/code/constants/","title":"Constants","text":""},{"location":"markdown/code/constants/#db2ixf.constants","title":"<code>constants</code>","text":"<p>Contains constants about mappers, schemas, metadata and others</p>"},{"location":"markdown/code/constants/#db2ixf.constants-attributes","title":"Attributes","text":""},{"location":"markdown/code/constants/#db2ixf.constants.HEADER_RECORD_TYPE","title":"<code>HEADER_RECORD_TYPE = OrderedDict({'IXFHRECL': 6, 'IXFHRECT': 1, 'IXFHID': 3, 'IXFHVERS': 4, 'IXFHPROD': 12, 'IXFHDATE': 8, 'IXFHTIME': 6, 'IXFHHCNT': 5, 'IXFHSBCP': 5, 'IXFHDBCP': 5, 'IXFHFIL1': 2})</code>  <code>module-attribute</code>","text":"<p>Length in bytes of the fields in the header record.</p>"},{"location":"markdown/code/constants/#db2ixf.constants.TABLE_RECORD_TYPE","title":"<code>TABLE_RECORD_TYPE = OrderedDict({'IXFTRECL': 6, 'IXFTRECT': 1, 'IXFTNAML': 3, 'IXFTNAME': 256, 'IXFTQULL': 3, 'IXFTQUAL': 256, 'IXFTSRC': 12, 'IXFTDATA': 1, 'IXFTFORM': 1, 'IXFTMFRM': 5, 'IXFTLOC': 1, 'IXFTCCNT': 5, 'IXFTFIL1': 2, 'IXFTDESC': 30, 'IXFTPKNM': 257, 'IXFTDSPC': 257, 'IXFTISPC': 257, 'IXFTLSPC': 257})</code>  <code>module-attribute</code>","text":"<p>Length in bytes of the fields in the table record.</p>"},{"location":"markdown/code/constants/#db2ixf.constants.COL_DESCRIPTOR_RECORD_TYPE","title":"<code>COL_DESCRIPTOR_RECORD_TYPE = OrderedDict({'IXFCRECL': 6, 'IXFCRECT': 1, 'IXFCNAML': 3, 'IXFCNAME': 256, 'IXFCNULL': 1, 'IXFCDEF': 1, 'IXFCSLCT': 1, 'IXFCKPOS': 2, 'IXFCCLAS': 1, 'IXFCTYPE': 3, 'IXFCSBCP': 5, 'IXFCDBCP': 5, 'IXFCLENG': 5, 'IXFCDRID': 3, 'IXFCPOSN': 6, 'IXFCDESC': 30, 'IXFCLOBL': 20, 'IXFCUDTL': 3, 'IXFCUDTN': 256, 'IXFCDEFL': 3, 'IXFCDEFV': 254, 'IXFCREF': 1, 'IXFCNDIM': 2})</code>  <code>module-attribute</code>","text":"<p>Length in bytes of the fields in the column descriptor record.</p>"},{"location":"markdown/code/constants/#db2ixf.constants.DATA_RECORD_TYPE","title":"<code>DATA_RECORD_TYPE = OrderedDict({'IXFDRECL': 6, 'IXFDRECT': 1, 'IXFDRID': 3, 'IXFDFIL1': 4})</code>  <code>module-attribute</code>","text":"<p>Length in bytes of the fields in the data record.</p>"},{"location":"markdown/code/constants/#db2ixf.constants.APPLICATION_RECORD_TYPE","title":"<code>APPLICATION_RECORD_TYPE = OrderedDict({'IXFARECL': 6, 'IXFARECT': 1, 'IXFAPPID': 12})</code>  <code>module-attribute</code>","text":"<p>Length in bytes of the fields in the application record.</p>"},{"location":"markdown/code/constants/#db2ixf.constants.IXF_DTYPES","title":"<code>IXF_DTYPES = {384: 'DATE', 388: 'TIME', 392: 'TIMESTAMP', 404: 'BLOB', 408: 'CLOB', 412: 'DBCLOB', 448: 'VARCHAR', 452: 'CHAR', 456: 'LONGVARCHAR', 464: 'VARGRAPHIC', 468: 'GRAPHIC', 472: 'LONG VARGRAPHIC', 480: 'FLOATING POINT', 484: 'DECIMAL', 492: 'BIGINT', 496: 'INTEGER', 500: 'SMALLINT', 908: 'VARBINARY', 912: 'BINARY', 916: 'BLOB_FILE', 920: 'CLOB_FILE', 924: 'DBCLOB_FILE', 996: 'DECFLOAT'}</code>  <code>module-attribute</code>","text":"<p>IXF data types</p>"},{"location":"markdown/code/constants/#db2ixf.constants.DB2IXF_ACCEPTED_CORRUPTION_RATE","title":"<code>DB2IXF_ACCEPTED_CORRUPTION_RATE: int = int(os.getenv('DB2IXF_ACCEPTED_CORRUPTION_RATE', 1))</code>  <code>module-attribute</code>","text":"<p>Accepted rate of corrupted data, attention to data loss !</p>"},{"location":"markdown/code/constants/#db2ixf.constants.MAX_SIZE_IXF_DATA_RECORD","title":"<code>MAX_SIZE_IXF_DATA_RECORD: int = 32 * 1024</code>  <code>module-attribute</code>","text":"<p>See IBM Doc: Max size of the data area of a data record in ixf format is  around 32 KB.</p>"},{"location":"markdown/code/constants/#db2ixf.constants.DB2IXF_BUFFER_SIZE_CLOUD_PROVIDER","title":"<code>DB2IXF_BUFFER_SIZE_CLOUD_PROVIDER: int = int(os.getenv('BUFFER_SIZE_CLI_CLOUD_PROVIDER', 4 * 1024 * 1024))</code>  <code>module-attribute</code>","text":"<p>Buffer size of clients of cloud providers storage services</p>"},{"location":"markdown/code/constants/#db2ixf.constants.DB2IXF_DEFAULT_BATCH_SIZE","title":"<code>DB2IXF_DEFAULT_BATCH_SIZE: int = int(os.getenv('DB2IXF_DEFAULT_BATCH_SIZE', int(DB2IXF_BUFFER_SIZE_CLOUD_PROVIDER / MAX_SIZE_IXF_DATA_RECORD)))</code>  <code>module-attribute</code>","text":"<p>Batch size (number of rows), defaults to 128</p>"},{"location":"markdown/code/constants/#db2ixf.constants.DB2IXF_RISK_FACTOR","title":"<code>DB2IXF_RISK_FACTOR: int = int(os.getenv('DB2IXF_RISK_FACTOR', 1.9))</code>  <code>module-attribute</code>","text":"<p>Risk factor of the dynamic batch size (Standard deviation of Normal dist)</p>"},{"location":"markdown/code/constants/#db2ixf.constants.DB2IXF_TIME_ZONE","title":"<code>DB2IXF_TIME_ZONE = os.getenv('DB2IXF_TIME_ZONE')</code>  <code>module-attribute</code>","text":"<p>Time zone where the db2 server is hosted or the one used when extracting the  ixf file. Default <code>None</code> means all timestamps are considered time zone naive.</p>"},{"location":"markdown/code/db2ixf/","title":"IXF","text":""},{"location":"markdown/code/db2ixf/#db2ixf","title":"<code>db2ixf</code>","text":"<p>Helps the user to parse PC/IXF file format of IBM DB2.</p> <p>IXF file is organised in a sequence of records. these records have 5 main types: Header, Table, Column Descriptor, Data and Application.</p> <p>Inside the IXF file, these records are ordered which means that it starts with a header record, table one, set of column descriptors - where each column descriptor is also a record - ant it ends with the set of data records.</p> <p>IXF = H + T + Set(C) + Set(D).</p> <p>Each record type is represented by a list of fields and each field has a length in bytes that we will use to read data from the IXF file.</p> <p>For more information about record types; Please visit this link.</p> <p>Data records [Set(D)] stores the data we want to extract, which means that for each column we need to extract its content from the data record. Each column has its data type.</p> <p>For more information about data types; Please visit this link.</p>"},{"location":"markdown/code/db2ixf/#db2ixf-classes","title":"Classes","text":""},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser","title":"<code>IXFParser(file)</code>","text":"<p>PC/IXF Parser.</p> <p>Attributes:</p> Name Type Description <code>file</code> <code>str, Path, PathLike or File-Like Object</code> <p>Input file and it is better to use file-like object.</p> <p>Init an instance of the PC/IXF Parser.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str, Path, PathLike or File-Like Object</code> <p>Input file and it is better to use file-like object.</p> required Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __init__(self, file: Union[str, Path, PathLike, BinaryIO]):\n    \"\"\"Init an instance of the PC/IXF Parser.\n\n    Parameters\n    ----------\n    file : str, Path, PathLike or File-Like Object\n        Input file and it is better to use file-like object.\n    \"\"\"\n    if isinstance(file, (str, Path, PathLike)):\n        file = open(file, mode=\"rb\")\n        logger.debug(\"File opened in read &amp; binary mode\")\n\n    if file.mode != \"rb\":\n        msg = \"file-like object should be opened in read-binary mode\"\n        raise ValueError(msg)\n\n    # Init instance attributes\n    self.file = file\n\n    # State\n    self.file_size: int = get_filesize(file)\n    logger.debug(f\"File size = {self.file_size} bytes\")\n    \"\"\"IXF file size\"\"\"\n    self.header_record: OrderedDict = OrderedDict()\n    \"\"\"Contains header metadata extracted from the ixf file.\"\"\"\n    self.table_record: OrderedDict = OrderedDict()\n    \"\"\"Contains table metadata extracted from the ixf file.\"\"\"\n    self.column_records: List[OrderedDict] = []\n    \"\"\"Contains columns description extracted from the ixf file.\"\"\"\n    self.pyarrow_schema: Schema = schema([])\n    \"\"\"Pyarrow schema extracted from the ixf file.\"\"\"\n    self.current_data_record: OrderedDict = OrderedDict()\n    \"\"\"Contains current data record extracted from ixf file.\"\"\"\n    self.end_data_records: bool = False\n    \"\"\"Flag the end of the data records in the ixf file.\"\"\"\n    self.current_row: OrderedDict = OrderedDict()\n    \"\"\"Contains parsed data extracted from a data record of the ixf file.\"\"\"\n    self.current_row_size: int = 0\n    \"\"\"Current row size in bytes\"\"\"\n    self.current_total_size: int = 0\n    \"\"\"Current total size of the rows\"\"\"\n    self.number_rows: int = 0\n    \"\"\"Number of rows extracted from the ixf file.\"\"\"\n    # Avoids counting the last line (EOF)\n    self.number_corrupted_rows: int = -1\n    \"\"\"Number of corrupted rows in the ixf file.\"\"\"\n    self.opt_batch_size: int = init_opt_batch_size(self.file_size)\n    \"\"\"Estimated optimal batch size\"\"\"\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser-attributes","title":"Attributes","text":""},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.header_record","title":"<code>header_record: OrderedDict = OrderedDict()</code>  <code>instance-attribute</code>","text":"<p>Contains header metadata extracted from the ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.table_record","title":"<code>table_record: OrderedDict = OrderedDict()</code>  <code>instance-attribute</code>","text":"<p>Contains table metadata extracted from the ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.column_records","title":"<code>column_records: List[OrderedDict] = []</code>  <code>instance-attribute</code>","text":"<p>Contains columns description extracted from the ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.pyarrow_schema","title":"<code>pyarrow_schema: Schema = schema([])</code>  <code>instance-attribute</code>","text":"<p>Pyarrow schema extracted from the ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.current_data_record","title":"<code>current_data_record: OrderedDict = OrderedDict()</code>  <code>instance-attribute</code>","text":"<p>Contains current data record extracted from ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.end_data_records","title":"<code>end_data_records: bool = False</code>  <code>instance-attribute</code>","text":"<p>Flag the end of the data records in the ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.current_row","title":"<code>current_row: OrderedDict = OrderedDict()</code>  <code>instance-attribute</code>","text":"<p>Contains parsed data extracted from a data record of the ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.current_row_size","title":"<code>current_row_size: int = 0</code>  <code>instance-attribute</code>","text":"<p>Current row size in bytes</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.current_total_size","title":"<code>current_total_size: int = 0</code>  <code>instance-attribute</code>","text":"<p>Current total size of the rows</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.number_rows","title":"<code>number_rows: int = 0</code>  <code>instance-attribute</code>","text":"<p>Number of rows extracted from the ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.number_corrupted_rows","title":"<code>number_corrupted_rows: int = -1</code>  <code>instance-attribute</code>","text":"<p>Number of corrupted rows in the ixf file.</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.opt_batch_size","title":"<code>opt_batch_size: int = init_opt_batch_size(self.file_size)</code>  <code>instance-attribute</code>","text":"<p>Estimated optimal batch size</p>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser-functions","title":"Functions","text":""},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__read_header","title":"<code>__read_header(record_type=None)</code>","text":"<p>Read the header record.</p> <p>Parameters:</p> Name Type Description Default <code>record_type</code> <code>dict</code> <p>Dictionary containing the names of the record fields and their length.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Header record of the input file.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __read_header(\n    self,\n    record_type: OrderedDict = None\n) -&gt; OrderedDict:\n    \"\"\"Read the header record.\n\n    Parameters\n    ----------\n    record_type : dict\n        Dictionary containing the names of the record fields and\n        their length.\n\n    Returns\n    -------\n    dict\n        Header record of the input file.\n    \"\"\"\n    if record_type is None:\n        record_type = HEADER_RECORD_TYPE\n\n    for u, w in record_type.items():\n        self.header_record[u] = self.file.read(w)\n\n    return self.header_record\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__read_table","title":"<code>__read_table(record_type=None)</code>","text":"<p>Read the table record.</p> <p>Parameters:</p> Name Type Description Default <code>record_type</code> <code>dict</code> <p>Dictionary containing the names of the record fields and their length.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Table record of the input file.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __read_table(\n    self,\n    record_type: OrderedDict = None\n) -&gt; OrderedDict:\n    \"\"\"Read the table record.\n\n    Parameters\n    ----------\n    record_type : dict\n        Dictionary containing the names of the record fields and\n        their length.\n\n    Returns\n    -------\n    dict\n        Table record of the input file.\n    \"\"\"\n    if record_type is None:\n        record_type = TABLE_RECORD_TYPE\n\n    for m, n in record_type.items():\n        self.table_record[m] = self.file.read(n)\n\n    return self.table_record\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__read_column_records","title":"<code>__read_column_records(record_type=None)</code>","text":"<p>Read the column records.</p> <p>Parameters:</p> Name Type Description Default <code>record_type</code> <code>dict</code> <p>Dictionary containing the names of the record fields and their length.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[OrderedDict]</code> <p>Column descriptors records of the input file.</p> <p>Raises:</p> Type Description <code>NotValidColumnDescriptorException</code> <p>If the IXF contains a non valid column descriptor.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __read_column_records(\n    self,\n    record_type: OrderedDict = None\n) -&gt; List[OrderedDict]:\n    \"\"\"Read the column records.\n\n    Parameters\n    ----------\n    record_type : dict\n        Dictionary containing the names of the record fields and\n        their length.\n\n    Returns\n    -------\n    List[OrderedDict]\n        Column descriptors records of the input file.\n\n    Raises\n    ------\n    NotValidColumnDescriptorException\n        If the IXF contains a non valid column descriptor.\n    \"\"\"\n    if record_type is None:\n        record_type = COL_DESCRIPTOR_RECORD_TYPE\n\n    # \"IXFTCCNT\" contains number of columns in the table\n    for _ in range(0, int(self.table_record[\"IXFTCCNT\"])):\n        column = OrderedDict()\n        for i, j in record_type.items():\n            column[i] = self.file.read(j)\n\n        if column[\"IXFCRECT\"] != b\"C\":\n            msg1 = f\"Non valid IXF file: It either contains non \" \\\n                   f\"supported record type/subtype like application \" \\\n                   f\"one or it contains a non valid column descriptor \" \\\n                   f\"(see the column {column['IXFCNAME']}).\"\n            logger.error(msg1)\n            msg2 = \"Hint: try to recreate IXF file without any \" \\\n                   \"application record or any SQL error.\"\n            logger.info(msg2)\n            raise NotValidColumnDescriptorException(msg1)\n\n        column[\"IXFCDSIZ\"] = self.file.read(int(column[\"IXFCRECL\"]) - 862)\n\n        self.column_records.append(column)\n\n    return self.column_records\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__read_data_record","title":"<code>__read_data_record(record_type=None)</code>","text":"<p>Read one data record.</p> <p>Parameters:</p> Name Type Description Default <code>record_type</code> <code>dict</code> <p>Dictionary containing the names of the record fields and their length.</p> <code>None</code> <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Dictionary containing current data record from IXF file.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __read_data_record(\n    self,\n    record_type: OrderedDict = None\n) -&gt; OrderedDict:\n    \"\"\"Read one data record.\n\n    Parameters\n    ----------\n    record_type : dict\n        Dictionary containing the names of the record fields and\n        their length.\n\n    Returns\n    ------\n    OrderedDict\n        Dictionary containing current data record from IXF file.\n    \"\"\"\n    if record_type is None:\n        record_type = DATA_RECORD_TYPE\n\n    self.current_data_record = OrderedDict()\n    for key, val in record_type.items():\n        self.current_data_record[key] = self.file.read(val)\n\n    self.current_data_record[\"IXFDCOLS\"] = self.file.read(\n        int(self.current_data_record[\"IXFDRECL\"]) - 8\n    )\n    return self.current_data_record\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__parse_data_record","title":"<code>__parse_data_record()</code>","text":"<p>Parses one data record.</p> <p>It collects data from fields of the current data record.</p> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <code>OrderedDict</code> <p>Dictionary containing all extracted data from fields of the data record.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __parse_data_record(self) -&gt; OrderedDict:\n    \"\"\"Parses one data record.\n\n    It collects data from fields of the current data record.\n\n    Returns\n    -------\n    OrderedDict:\n        Dictionary containing all extracted data from fields of\n        the data record.\n    \"\"\"\n    # Start Extraction\n    try:\n        self.current_row = OrderedDict()\n        for c in self.column_records:\n            # Extract some metadata about the column\n            col_name = str(c[\"IXFCNAME\"], encoding=\"utf-8\").strip()\n            col_type = int(c[\"IXFCTYPE\"])\n            col_is_nullable = c[\"IXFCNULL\"] == b\"Y\"\n            col_position = int(c[\"IXFCPOSN\"])\n\n            # Init the data collection\n            self.current_row[col_name] = None\n            collected_data = None  # noqa\n\n            # Parse next data record in case a column is in position 1\n            if col_position == 1:\n                self.__read_data_record()\n\n            # Mark the end of data records: helps exit the while loop\n            if self.current_data_record[\"IXFDRECT\"] != b\"D\":\n                self.end_data_records = True\n                self.current_row = OrderedDict()\n                logger.debug(\"End of data records\")\n                break\n\n            # Position index is then equals to position - 1\n            pos = col_position - 1\n\n            # Handle nullable\n            if col_is_nullable:\n                # Column is null\n                _dr = self.current_data_record[\"IXFDCOLS\"][pos:pos + 2]\n                if _dr == b\"\\xff\\xff\":\n                    self.current_row[col_name] = None\n                    continue\n                # Column is not null\n                elif _dr == b\"\\x00\\x00\":\n                    pos += 2\n\n            # Collect data\n            collector = collectors.get(col_type, None)\n            if collector is None:\n                msg = f\"The column {col_name} has unknown \" \\\n                      f\"data type {col_type}\"\n                raise UnknownDataTypeException(msg)\n\n            collected_data = collector(\n                c, self.current_data_record[\"IXFDCOLS\"], pos\n            )\n            self.current_row[col_name] = collected_data\n\n        self.current_data_record = OrderedDict()\n        return self.current_row\n    except DataCollectorError as er1:\n        logger.error(er1)\n        self.current_row = OrderedDict()\n        return self.current_row\n    except (UnknownDataTypeException, Exception) as er2:\n        logger.error(er2)\n        self.current_row = OrderedDict()\n        raise IXFParsingError(er2)\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__update_statistics","title":"<code>__update_statistics()</code>","text":"<p>Update stats and change state of the parser</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __update_statistics(self) -&gt; \"IXFParser\":\n    \"\"\"Update stats and change state of the parser\"\"\"\n    # Stats calculation\n    self.number_rows += 1\n    self.current_row_size = sys.getsizeof(self.current_row)\n    self.current_total_size += self.current_row_size\n    if self.number_rows == 0:\n        self.estimated_row_size = self.current_row_size\n    else:\n        self.estimated_row_size = self.current_total_size \\\n                                  / self.number_rows\n\n    self.opt_batch_size = get_opt_batch_size(\n        self.opt_batch_size,\n        self.estimated_row_size\n    )\n\n    return self\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__parse_all_data_records","title":"<code>__parse_all_data_records()</code>","text":"<p>Parses all the data records.</p> <p>Yields:</p> Type Description <code>dict</code> <p>Parsed row data from IXF file.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __parse_all_data_records(self) -&gt; Iterable[OrderedDict]:\n    \"\"\"Parses all the data records.\n\n    Yields\n    ------\n    dict\n        Parsed row data from IXF file.\n    \"\"\"\n    # Start parsing\n    while not self.end_data_records:\n        # Extract data\n        self.__parse_data_record()\n\n        # Do not accept empty dictionary\n        if not self.current_row:\n            self.number_corrupted_rows += 1\n            continue\n\n        self.__update_statistics()\n        yield self.current_row\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__start_parsing","title":"<code>__start_parsing()</code>","text":"<p>Starts the parsing.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __start_parsing(self) -&gt; \"IXFParser\":\n    \"\"\"Starts the parsing.\"\"\"\n    logger.debug(\"Start parsing\")\n    logger.debug(\"Put the pointer at the beginning of the ixf file\")\n    self.file.seek(0)\n    logger.debug(\"Parse header record\")\n    self.__read_header()\n    logger.debug(\"Parse table record\")\n    self.__read_table()\n    logger.debug(\"Parse column descriptor records\")\n    self.__read_column_records()\n    return self\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.start_parsing","title":"<code>start_parsing()</code>","text":"<p>Starts the parsing.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def start_parsing(self) -&gt; \"IXFParser\":\n    \"\"\"Starts the parsing.\"\"\"\n    return self.__start_parsing()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__check_parsing","title":"<code>__check_parsing()</code>","text":"<p>Do some checks on the parsing.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __check_parsing(self) -&gt; bool:\n    \"\"\"Do some checks on the parsing.\"\"\"\n    total_rows = self.number_corrupted_rows + self.number_rows\n    if total_rows == 0:\n        logger.warning(\"Empty ixf file\")\n        self.file.close()\n        return True\n\n    logger.debug(f\"Number of total rows = {total_rows}\")\n    logger.debug(f\"Number of healthy rows = {self.number_rows}\")\n    logger.debug(f\"Number of corrupted rows = {self.number_corrupted_rows}\")\n\n    cor_rate = self.number_corrupted_rows / total_rows * 100\n\n    if int(cor_rate) != 0:\n        logger.warning(f\"Corrupted ixf file (rate={cor_rate}%)\")\n\n    if int(cor_rate) &gt; DB2IXF_ACCEPTED_CORRUPTION_RATE:\n        _msg = f\"Corrupted data ({cor_rate}%) &gt; \" \\\n               f\"({DB2IXF_ACCEPTED_CORRUPTION_RATE}%) accepted rate\"\n        logger.error(_msg)\n        logger.warning(\n            \"You can change the accepted rate of the corrupted data \"\n            \"by setting `DB2IXF_ACCEPTED_CORRUPTION_RATE` environment \"\n            \"variable to a higher value\"\n        )\n        self.file.close()\n        raise IXFParsingError(_msg)\n\n    self.file.close()\n    return True\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.check_parsing","title":"<code>check_parsing()</code>","text":"<p>Do some checks on the parsing.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if parsing and/or conversion are ok.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def check_parsing(self) -&gt; bool:\n    \"\"\"Do some checks on the parsing.\n\n    Returns\n    -------\n    bool\n        True if parsing and/or conversion are ok.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    return self.__check_parsing()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__iter_row","title":"<code>__iter_row()</code>","text":"<p>Yields extracted rows (Without parsing of header, table, cols).</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __iter_row(self) -&gt; Iterable[Dict]:\n    \"\"\"Yields extracted rows (Without parsing of header, table, cols).\"\"\"\n    logger.debug(\"Parse all data records\")\n    for r in self.__parse_all_data_records():\n        yield dict(r)\n    logger.debug(\"Finished parsing\")\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.iter_row","title":"<code>iter_row()</code>","text":"<p>Yields parsed rows.</p> <p>It won\u2019t work if you use it alone. you need to start parsing with <code>start_parsing</code> method then you can iterate over rows using <code>iter_row</code>. Most of the time, you do not need to use this method.</p> <p>You will need it in case you want to customize the parsing for example adding support for a new output format.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def iter_row(self) -&gt; Iterable[Dict]:\n    \"\"\"Yields parsed rows.\n\n    It won't work if you use it alone. you need to start parsing with\n    `start_parsing` method then you can iterate over rows using `iter_row`.\n    Most of the time, you do not need to use this method.\n\n    You will need it in case you want to customize the parsing for\n    example adding support for a new output format.\n    \"\"\"\n    return self.__iter_row()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__iter_batch_of_rows","title":"<code>__iter_batch_of_rows(data=None, batch_size=None)</code>","text":"<p>Yields batch of parsed rows.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __iter_batch_of_rows(\n    self,\n    data: Optional[Iterable[Dict]] = None,\n    batch_size: Optional[int] = None\n) -&gt; Iterable[List[Dict]]:\n    \"\"\"Yields batch of parsed rows.\"\"\"\n    if data is None:\n        data = self.__iter_row()\n\n    if not isinstance(data, Iterable):\n        raise TypeError(f\"Expecting an `Iterable`, Got: {type(data)}\")\n\n    if batch_size is None:\n        _size = self.opt_batch_size\n    else:\n        _size = batch_size\n\n    if not isinstance(_size, int):\n        TypeError(f\"Expecting an `Integer`, Got {type(_size)}\")\n\n    batch = []\n    counter = 0\n    for i, row in enumerate(data):\n        batch.append(row)\n        counter += 1\n        if counter % _size == 0:\n            _size = batch_size if batch_size else self.opt_batch_size\n            yield batch\n            batch = []\n\n    # Yield the remaining rows as the last batch\n    if batch:\n        yield batch\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.iter_batch_of_rows","title":"<code>iter_batch_of_rows(data=None, batch_size=None)</code>","text":"<p>Yields batches of parsed rows.</p> <p>It won\u2019t work if you use it alone. you need to start parsing with <code>start_parsing</code> method then you can iterate over batch of rows using <code>iter_batch_of_rows</code> method. Most of the time, you do not need to use this method.</p> <p>You will need it in case you want to customize the parsing for example adding support for a new output format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable[Dict]</code> <p>Data extracted from ixf file (parsed rows).</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <p>Yields:</p> Type Description <code>List[Dict]</code> <p>Batch of </p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def iter_batch_of_rows(\n    self,\n    data: Optional[Iterable[Dict]] = None,\n    batch_size: Optional[int] = None\n) -&gt; Iterable[List[Dict]]:\n    \"\"\"Yields batches of parsed rows.\n\n    It won't work if you use it alone. you need to start parsing with\n    `start_parsing` method then you can iterate over batch of rows using\n    `iter_batch_of_rows` method. Most of the time, you do not need to use\n    this method.\n\n    You will need it in case you want to customize the parsing for\n    example adding support for a new output format.\n\n    Parameters\n    ----------\n    data : Iterable[Dict]\n        Data extracted from ixf file (parsed rows).\n    batch_size : int\n        Batch size.\n\n    Yields\n    ------\n    List[Dict]\n        Batch of \n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    batches = self.__iter_batch_of_rows(\n        data=data,\n        batch_size=batch_size\n    )\n    for batch in batches:\n        yield batch\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__iter_pyarrow_record_batch","title":"<code>__iter_pyarrow_record_batch(data=None, batch_size=None)</code>","text":"<p>Yields pyarrow record batches from an iterable of rows.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __iter_pyarrow_record_batch(\n    self,\n    data: Optional[Iterable[Dict]] = None,\n    batch_size: Optional[int] = None,\n) -&gt; Iterable[RecordBatch]:\n    \"\"\"Yields pyarrow record batches from an iterable of rows.\"\"\"\n    if data is None:\n        data = self.__iter_row()\n\n    if not isinstance(data, Iterable):\n        raise TypeError(f\"Expecting an `Iterable`, Got: {type(data)}\")\n\n    if batch_size is None:\n        _size = self.opt_batch_size\n    else:\n        _size = batch_size\n\n    if not isinstance(_size, int):\n        TypeError(f\"Expecting an `Integer`, Got {type(_size)}\")\n\n    batch = defaultdict(list)\n    counter = 0\n    for i, row in enumerate(data):\n        for key, value in row.items():\n            batch[key].append(value)\n        counter += 1\n        if counter % _size == 0:\n            _size = batch_size if batch_size else self.opt_batch_size\n            yield to_pyarrow_record_batch(batch, self.pyarrow_schema)\n            batch = defaultdict(list)\n\n    if batch:\n        yield to_pyarrow_record_batch(batch, self.pyarrow_schema)\n        batch.clear()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.iter_pyarrow_record_batch","title":"<code>iter_pyarrow_record_batch(data=None, batch_size=None)</code>","text":"<p>Yields pyarrow record batches.</p> <p>It won\u2019t work if you use it alone. you need to start parsing with <code>start_parsing</code> method, create the pyarrow schema using <code>get_or_create_pyarrow_schema</code> method then you can iterate over record batches using <code>iter_batch_of_rows</code> method. Most of the time, you do not need to use this method.</p> <p>You will need it in case you want to customize the parsing for example adding support for a new output format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable[Dict]</code> <p>Data extracted from ixf file (parsed rows).</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <p>Yields:</p> Type Description <code>RecordBatch</code> <p>Pyarrow record batch.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def iter_pyarrow_record_batch(\n    self,\n    data: Optional[Iterable[Dict]] = None,\n    batch_size: Optional[int] = None,\n) -&gt; Iterable[RecordBatch]:\n    \"\"\"Yields pyarrow record batches.\n\n    It won't work if you use it alone. you need to start parsing with\n    `start_parsing` method, create the pyarrow schema using\n    `get_or_create_pyarrow_schema` method then you can iterate over record\n    batches using `iter_batch_of_rows` method. Most of the time, you do not\n    need to use this method.\n\n    You will need it in case you want to customize the parsing for\n    example adding support for a new output format.\n\n    Parameters\n    ----------\n    data : Iterable[Dict]\n        Data extracted from ixf file (parsed rows).\n    batch_size : int\n        Batch size.\n\n    Yields\n    ------\n    RecordBatch\n        Pyarrow record batch.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    batches = self.__iter_pyarrow_record_batch(\n        data=data,\n        batch_size=batch_size,\n    )\n\n    for batch in batches:\n        yield batch\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.__get_or_create_pyarrow_schema","title":"<code>__get_or_create_pyarrow_schema(pyarrow_schema=None, for_delta=False)</code>","text":"<p>Get or create pyarrow schema based on the scope it will be used.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def __get_or_create_pyarrow_schema(\n    self,\n    pyarrow_schema: Optional[Schema] = None,\n    for_delta: Optional[bool] = False\n) -&gt; Schema:\n    \"\"\"Get or create pyarrow schema based on the scope it will be used.\"\"\"\n    if pyarrow_schema is None:\n        logger.debug(\"Get pyarrow schema from column records\")\n        pyarrow_schema = get_pyarrow_schema(self.column_records)\n\n    if for_delta:\n        logger.debug(\n            \"Apply fixes on pyarrow schema for deltalake adaptation\"\n        )\n        pyarrow_schema = apply_schema_fixes(pyarrow_schema)\n\n    self.pyarrow_schema = pyarrow_schema\n\n    return self.pyarrow_schema\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.get_or_create_pyarrow_schema","title":"<code>get_or_create_pyarrow_schema(pyarrow_schema=None, for_delta=False)</code>","text":"<p>Get or create pyarrow schema based on the scope of the usage.</p> <p>It won\u2019t work if you use it alone. you need to start parsing with <code>start_parsing</code> method then you can create the pyarrow schema using <code>get_or_create_pyarrow_schema</code> method. After applying it, you will maybe need to iterate over pyarrow record batches. Most of the time, you do not need to use this method.</p> <p>You will need it in case you want to customize the parsing for example adding support for a new output format.</p> <p>Parameters:</p> Name Type Description Default <code>pyarrow_schema</code> <code>Schema</code> <p>Pyarrow schema.</p> <code>None</code> <code>for_delta</code> <code>bool</code> <p>If True, it adapts pyarrow schema for deltalake usage.</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>Pyarrow schema</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def get_or_create_pyarrow_schema(\n    self,\n    pyarrow_schema: Optional[Schema] = None,\n    for_delta: Optional[bool] = False\n) -&gt; Schema:\n    \"\"\"Get or create pyarrow schema based on the scope of the usage.\n\n    It won't work if you use it alone. you need to start parsing with\n    `start_parsing` method then you can create the pyarrow schema using\n    `get_or_create_pyarrow_schema` method. After applying it, you will maybe\n    need to iterate over pyarrow record batches. Most of the time, you do\n    not need to use this method.\n\n    You will need it in case you want to customize the parsing for\n    example adding support for a new output format.\n\n    Parameters\n    ----------\n    pyarrow_schema : Schema\n        Pyarrow schema.\n    for_delta : bool\n        If True, it adapts pyarrow schema for deltalake usage.\n\n    Returns\n    -------\n    Schema\n        Pyarrow schema\n    \"\"\"\n    _schema = self.__get_or_create_pyarrow_schema(\n        pyarrow_schema=pyarrow_schema,\n        for_delta=for_delta\n    )\n    return _schema\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.get_row","title":"<code>get_row()</code>","text":"<p>Yields parsed rows.</p> <p>Yields:</p> Type Description <code>Dict</code> <p>Generated parsed row.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def get_row(self) -&gt; Iterable[Dict]:\n    \"\"\"Yields parsed rows.\n\n    Yields\n    ------\n    Dict\n        Generated parsed row.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    self.__start_parsing()\n    for r in self.__iter_row():\n        yield r\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.parse","title":"<code>parse()</code>","text":"<p>Yields parsed rows.</p> <p>Alias for <code>get_row</code> for compatibility with old versions.</p> <p>Yields:</p> Type Description <code>Dict</code> <p>Generated parsed row.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>@deprecated(\"0.16.0\", \"Use `get_row` method instead of `parse`.\")\ndef parse(self) -&gt; Iterable[Dict]:\n    \"\"\"Yields parsed rows.\n\n    Alias for `get_row` for compatibility with old versions.\n\n    Yields\n    ------\n    Dict\n        Generated parsed row.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    return self.get_row()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.get_all_rows","title":"<code>get_all_rows()</code>","text":"<p>Get all the parsed rows from the ixf file.</p> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of all extracted rows.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Notes <ul> <li>Attention: it loads all the extracted rows into memory.</li> </ul> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def get_all_rows(self) -&gt; List[Dict]:\n    \"\"\"Get all the parsed rows from the ixf file.\n\n    Returns\n    -------\n    List[Dict]\n        List of all extracted rows.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n\n    Notes\n    -----\n    - Attention: it loads all the extracted rows into memory.\n    \"\"\"\n    rows = []\n    for row in self.get_row():\n        rows.append(row)\n\n    if self.__check_parsing() is True:\n        return rows\n    return rows\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.to_json","title":"<code>to_json(output)</code>","text":"<p>Parses and converts to JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, Path, PathLike, IO]</code> <p>Output file. It is better to use file-like object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the parsing and conversion are ok.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def to_json(\n    self,\n    output: Union[str, Path, PathLike, TextIO]\n) -&gt; bool:\n    \"\"\"Parses and converts to JSON format.\n\n    Parameters\n    ----------\n    output : Union[str, Path, PathLike, IO]\n        Output file. It is better to use file-like object.\n\n    Returns\n    -------\n    bool\n        True if the parsing and conversion are ok.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    if isinstance(output, (str, Path, PathLike)):\n        output = open(output, mode=\"w\", encoding=\"utf-8\")\n\n    if not hasattr(output, \"mode\"):\n        msg = \"File-like object should have `mode` attribute\"\n        raise TypeError(msg)\n\n    if output.mode not in [\"w\", \"wt\"]:\n        msg = \"File-like object should be opened in write and text mode\"\n        raise ValueError(msg)\n\n    # Force utf-8 encoding for the json file\n    # (Maybe we will need to log without forcing)\n    if output.encoding != \"utf-8\":\n        raise ValueError(\"File-like object should be `utf-8` encoded\")\n\n    # init the parsing\n    self.__start_parsing()\n    _rows = self.__iter_row()\n\n    logger.debug(\"Start writing in the json file\")\n    with output as out:\n        out.write(\"[\")\n        first_row = True\n        for r in _rows:\n            if not first_row:\n                out.write(\",\")\n            json.dump(r, out, ensure_ascii=False, cls=CustomJSONEncoder)\n            first_row = False\n        out.write(\"]\")\n    logger.debug(\"Finished writing json file\")\n\n    # dereference source data\n    del _rows\n\n    return self.__check_parsing()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.to_jsonline","title":"<code>to_jsonline(output)</code>","text":"<p>Parses and converts to JSON LINE format.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, Path, PathLike, IO]</code> <p>Output file. It is better to use file-like object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the parsing and conversion are ok.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def to_jsonline(\n    self,\n    output: Union[str, Path, PathLike, TextIO]\n) -&gt; bool:\n    \"\"\"Parses and converts to JSON LINE format.\n\n    Parameters\n    ----------\n    output : Union[str, Path, PathLike, IO]\n        Output file. It is better to use file-like object.\n\n    Returns\n    -------\n    bool\n        True if the parsing and conversion are ok.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    if isinstance(output, (str, Path, PathLike)):\n        output = open(output, mode=\"w\", encoding=\"utf-8\")\n\n    if not hasattr(output, \"mode\"):\n        msg = \"File-like object should have `mode` attribute\"\n        raise TypeError(msg)\n\n    if output.mode not in [\"w\", \"wt\"]:\n        msg = \"File-like object should be opened in write and text mode\"\n        raise ValueError(msg)\n\n    # Force utf-8 encoding for the json file\n    # (Maybe we will need to log without forcing)\n    if output.encoding != \"utf-8\":\n        raise ValueError(\"File-like object should be `utf-8` encoded\")\n\n    # init the parsing\n    self.__start_parsing()\n    _rows = self.__iter_row()\n\n    logger.debug(\"Start writing in the json line file\")\n    with output as out:\n        for r in _rows:\n            json.dump(r, out, ensure_ascii=False, cls=CustomJSONEncoder)\n            out.write(\"\\n\")\n    logger.debug(\"Finished writing json line file\")\n\n    # dereference source data\n    del _rows\n\n    return self.__check_parsing()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.to_csv","title":"<code>to_csv(output, sep='|', batch_size=None)</code>","text":"<p>Parses and converts to CSV format.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, Path, PathLike, TextIO]</code> <p>Output file. It is better to use file-like object</p> required <code>sep</code> <code>str</code> <p>Separator/delimiter of the columns.</p> <code>'|'</code> <code>batch_size</code> <code>int</code> <p>Batch size, it used for memory optimization</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the parsing and conversion are ok</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def to_csv(\n    self,\n    output: Union[str, Path, PathLike, TextIO],\n    sep: Optional[str] = \"|\",\n    batch_size: Optional[int] = None\n) -&gt; bool:\n    \"\"\"Parses and converts to CSV format.\n\n    Parameters\n    ----------\n    output : Union[str, Path, PathLike, TextIO]\n        Output file. It is better to use file-like object\n    sep : str\n        Separator/delimiter of the columns.\n    batch_size : int\n        Batch size, it used for memory optimization\n\n    Returns\n    -------\n    bool\n        True if the parsing and conversion are ok\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    if isinstance(output, (str, Path, PathLike)):\n        output = open(output, mode=\"w\", encoding=\"utf-8\")\n\n    if not hasattr(output, \"mode\"):\n        raise TypeError(\"File-like object should have `mode` attribute\")\n\n    if output.mode not in [\"w\", \"wt\"]:\n        msg = \"File-like object should be opened in write and text mode\"\n        raise ValueError(msg)\n\n    # Force utf-8 encoding for the csv file\n    # (Maybe we only need to log without forcing)\n    if output.encoding != \"utf-8\":\n        raise ValueError(\"File-like object should be `utf-8` encoded\")\n\n    # init the parsing\n    self.__start_parsing()\n    batches = self.__iter_batch_of_rows(batch_size=batch_size)\n\n    logger.debug(\"Start writing in the csv file\")\n    with output as out:\n        writer = csv.writer(out, delimiter=sep)\n        writer.writerow(get_column_names(self.column_records))\n        for rows in batches:\n            writer.writerows([r.values() for r in rows])\n    logger.debug(\"Finished writing csv file\")\n\n    # dereference source data\n    del batches\n\n    return self.__check_parsing()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.get_pyarrow_record_batch","title":"<code>get_pyarrow_record_batch(data=None, batch_size=None, for_delta=False)</code>","text":"<p>Yields pyarrow records batches.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable[Dict]</code> <p>Data extracted from ixf file (parsed rows).</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <code>for_delta</code> <code>bool</code> <p>If True, it adapts pyarrow schema for deltalake usage.</p> <code>False</code> <p>Yields:</p> Type Description <code>RecordBatch</code> <p>Pyarrow record batch.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def get_pyarrow_record_batch(\n    self,\n    data: Optional[Iterable[Dict]] = None,\n    batch_size: Optional[int] = None,\n    for_delta: Optional[bool] = False\n) -&gt; Iterable[RecordBatch]:\n    \"\"\"Yields pyarrow records batches.\n\n    Parameters\n    ----------\n    data : Iterable[Dict]\n        Data extracted from ixf file (parsed rows).\n    batch_size : int\n        Batch size.\n    for_delta : bool\n        If True, it adapts pyarrow schema for deltalake usage.\n\n    Yields\n    ------\n    RecordBatch\n        Pyarrow record batch.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    self.__start_parsing()\n    self.pyarrow_schema = self.__get_or_create_pyarrow_schema(\n        for_delta=for_delta\n    )\n    batches = self.__iter_pyarrow_record_batch(\n        data=data,\n        batch_size=batch_size,\n    )\n    for batch in batches:\n        yield batch\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.to_parquet","title":"<code>to_parquet(output, parquet_version='2.6', batch_size=None)</code>","text":"<p>Parses and converts to PARQUET format.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[str, Path, PathLike, BinaryIO]</code> <p>Output file. It is better to use file-like object.</p> required <code>parquet_version</code> <code>str</code> <p>Parquet version. Please see pyarrow documentation.</p> <code>'2.6'</code> <code>batch_size</code> <code>int</code> <p>Number of rows to extract before writing to the parquet file. It is used for memory optimization.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the parsing and conversion are ok.</p> <p>Raises:</p> Type Description <code>IXFParsingError</code> <p>In case it encounters a parsing error.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def to_parquet(\n    self,\n    output: Union[str, Path, PathLike, BinaryIO],\n    parquet_version: str = \"2.6\",\n    batch_size: int = None\n) -&gt; bool:\n    \"\"\"Parses and converts to PARQUET format.\n\n    Parameters\n    ----------\n    output : Union[str, Path, PathLike, BinaryIO]\n        Output file. It is better to use file-like object.\n    parquet_version : str\n        Parquet version. Please see pyarrow documentation.\n    batch_size : int\n        Number of rows to extract before writing to the parquet file.\n        It is used for memory optimization.\n\n    Returns\n    -------\n    bool\n        True if the parsing and conversion are ok.\n\n    Raises\n    ------\n    IXFParsingError\n        In case it encounters a parsing error.\n    \"\"\"\n    if isinstance(output, (str, Path, PathLike)):\n        output = open(output, mode=\"wb\")\n\n    if not hasattr(output, \"mode\"):\n        raise TypeError(\"File-like object should have `mode` attribute\")\n\n    # Accept only write and text mode when opening output file\n    if output.mode != \"wb\":\n        msg = \"File-like object should be opened in write and binary mode\"\n        raise ValueError(msg)\n\n    # Init the parsing\n    self.__start_parsing()\n    self.pyarrow_schema = self.__get_or_create_pyarrow_schema()\n    batches = self.__iter_pyarrow_record_batch(batch_size=batch_size)\n\n    logger.debug(\"Start writing parquet file\")\n    with output as of:\n        with ParquetWriter(\n                where=of,\n                schema=self.pyarrow_schema,\n                flavor=\"spark\",\n                version=parquet_version\n        ) as writer:\n            for batch in batches:\n                writer.write_batch(batch)\n    logger.debug(\"Finished writing parquet file\")\n\n    # dereference source data\n    del batches\n\n    return self.__check_parsing()\n</code></pre>"},{"location":"markdown/code/db2ixf/#db2ixf.IXFParser.to_deltalake","title":"<code>to_deltalake(table_or_uri, partition_by=None, mode='error', overwrite_schema=False, schema_mode=None, partition_filters=None, large_dtypes=False, batch_size=None, **kwargs)</code>","text":"<p>Parses and converts to a deltalake table.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>Union[str, pathlib.Path, DeltaTable]</code> <p>URI of a table or a DeltaTable object.</p> required <code>partition_by</code> <code>Optional[Union[List[str], str]]</code> <p>List of columns to partition the table by. Only required when creating a new table.</p> <code>None</code> <code>mode</code> <code>Literal['error', 'append', 'overwrite', 'ignore']</code> <p>How to handle existing data. Default is to error if table already exists.     If \u201cappend\u201d, will add new data.     If \u201coverwrite\u201d, will replace table with new data.     If \u201cignore\u201d, will not write anything if table already exists.</p> <code>'error'</code> <code>overwrite_schema</code> <code>bool</code> <p>If True, allows updating the schema of the table.</p> <code>False</code> <code>schema_mode</code> <code>Optional[Literal['merge', 'overwrite']]</code> <p>If set to \u201coverwrite\u201d, allows replacing the schema of the table. Set to \u201cmerge\u201d to merge with existing schema.</p> <code>None</code> <code>partition_filters</code> <code>Optional[List[Tuple[str, str, Any]]]</code> <p>Defaults to None. The partition filters that will be used for partition overwrite. Only used in pyarrow engine.</p> <code>None</code> <code>large_dtypes</code> <code>bool</code> <p>If True, the table schema is checked against large_dtypes.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Number of rows to extract before conversion operation. It is used for memory optimization.</p> <code>None</code> <code>**kwargs</code> <code>Optional[dict]</code> <p>Some of the arguments you can give to this function <code>deltalake.write_deltalake</code>. See doc in https://delta-io.github.io/delta-rs/python/api_reference.html. Please, do not duplicate with the ones used in this function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the parsing and conversion are ok.</p> Source code in <code>src/db2ixf/ixf.py</code> Python<pre><code>def to_deltalake(\n    self,\n    table_or_uri: Union[str, Path, DeltaTable],\n    partition_by: Optional[Union[List[str], str]] = None,\n    mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"] = \"error\",\n    overwrite_schema: bool = False,\n    schema_mode: Optional[Literal[\"merge\", \"overwrite\"]] = None,\n    partition_filters: Optional[List[Tuple[str, str, Any]]] = None,\n    large_dtypes: bool = False,\n    batch_size: Optional[int] = None,\n    **kwargs\n) -&gt; bool:\n    \"\"\"Parses and converts to a deltalake table.\n\n    Parameters\n    ----------\n    table_or_uri : Union[str, pathlib.Path, DeltaTable]\n        URI of a table or a DeltaTable object.\n    partition_by : Optional[Union[List[str], str]]\n        List of columns to partition the table by. Only required when\n        creating a new table.\n    mode : Literal[\"error\", \"append\", \"overwrite\", \"ignore\"]\n        How to handle existing data.\n        Default is to error if table already exists.\n            If \"append\", will add new data.\n            If \"overwrite\", will replace table with new data.\n            If \"ignore\", will not write anything if table already exists.\n    overwrite_schema : bool\n        If True, allows updating the schema of the table.\n    schema_mode : Optional[Literal[\"merge\", \"overwrite\"]]\n        If set to \"overwrite\", allows replacing the schema of the table.\n        Set to \"merge\" to merge with existing schema.\n    partition_filters : Optional[List[Tuple[str, str, Any]]]\n        Defaults to None. The partition filters that will be used for\n        partition overwrite. Only used in pyarrow engine.\n    large_dtypes : bool\n        If True, the table schema is checked against large_dtypes.\n    batch_size : int\n        Number of rows to extract before conversion operation.\n        It is used for memory optimization.\n    **kwargs : Optional[dict]\n        Some of the arguments you can give to this function\n        `deltalake.write_deltalake`. See doc in\n        https://delta-io.github.io/delta-rs/python/api_reference.html.\n        Please, do not duplicate with the ones used in this function.\n\n    Returns\n    -------\n    bool:\n        True if the parsing and conversion are ok.\n    \"\"\"\n    # Init the parsing\n    self.__start_parsing()\n    self.pyarrow_schema = self.__get_or_create_pyarrow_schema(\n        for_delta=True\n    )\n    batches = self.__iter_pyarrow_record_batch(batch_size=batch_size)\n\n    logger.debug(\"Start writing to deltalake\")\n    deltalake.write_deltalake(\n        table_or_uri=table_or_uri,\n        data=batches,\n        schema=self.pyarrow_schema,\n        partition_by=partition_by,\n        mode=mode,\n        overwrite_schema=overwrite_schema,\n        schema_mode=schema_mode,\n        partition_filters=partition_filters,\n        large_dtypes=large_dtypes,\n        **kwargs\n    )\n    logger.debug(\"Finished writing to deltalake\")\n\n    # dereference source data\n    del batches\n\n    return self.__check_parsing()\n</code></pre>"},{"location":"markdown/code/encoders/","title":"Encoders","text":""},{"location":"markdown/code/encoders/#db2ixf.encoders","title":"<code>encoders</code>","text":"<p>Contains some encoders used to output data in some formats.</p>"},{"location":"markdown/code/encoders/#db2ixf.encoders-classes","title":"Classes","text":""},{"location":"markdown/code/encoders/#db2ixf.encoders.CustomJSONEncoder","title":"<code>CustomJSONEncoder</code>","text":"<p>Custom JSON encoder to handle python date, time and datetime objects.</p>"},{"location":"markdown/code/exceptions/","title":"Exceptions","text":""},{"location":"markdown/code/exceptions/#db2ixf.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for IXF parsing.</p>"},{"location":"markdown/code/exceptions/#db2ixf.exceptions-classes","title":"Classes","text":""},{"location":"markdown/code/exceptions/#db2ixf.exceptions.IXFParsingError","title":"<code>IXFParsingError</code>","text":"<p>Exception raised when facing issues with corrupted data of IXF file.</p>"},{"location":"markdown/code/exceptions/#db2ixf.exceptions.DataCollectorError","title":"<code>DataCollectorError</code>","text":"<p>Exception raised when facing issues with data collection from the IXF file. Read the doc: https://www.ibm.com/docs/en/db2/11.5?topic=format-pcixf-data-types</p>"},{"location":"markdown/code/exceptions/#db2ixf.exceptions.NotValidColumnDescriptorException","title":"<code>NotValidColumnDescriptorException</code>","text":"<p>Exception raised when encountering a non valid column descriptor. Read the doc: https://www.ibm.com/docs/en/db2/11.5?topic=format-pcixf-record-types</p>"},{"location":"markdown/code/exceptions/#db2ixf.exceptions.UnknownDataTypeException","title":"<code>UnknownDataTypeException</code>","text":"<p>Exception raised when encountering an unknown data type. Read the doc: https://www.ibm.com/docs/en/db2/11.5?topic=format-pcixf-data-types</p>"},{"location":"markdown/code/exceptions/#db2ixf.exceptions.NotValidDataPrecisionException","title":"<code>NotValidDataPrecisionException</code>","text":"<p>Exception raised when encountering a non valid data precision. Read the doc https://www.ibm.com/docs/en/db2/11.5?topic=format-pcixf-data-types</p>"},{"location":"markdown/code/helpers/","title":"Helpers","text":""},{"location":"markdown/code/helpers/#db2ixf.helpers","title":"<code>helpers</code>","text":"<p>Create helper function for schema generation and others.</p>"},{"location":"markdown/code/helpers/#db2ixf.helpers-attributes","title":"Attributes","text":""},{"location":"markdown/code/helpers/#db2ixf.helpers-classes","title":"Classes","text":""},{"location":"markdown/code/helpers/#db2ixf.helpers-functions","title":"Functions","text":""},{"location":"markdown/code/helpers/#db2ixf.helpers.init_opt_batch_size","title":"<code>init_opt_batch_size(file_size)</code>","text":"<p>Init optimal batch size</p> Source code in <code>src/db2ixf/helpers.py</code> Python<pre><code>def init_opt_batch_size(file_size: int):\n    \"\"\"Init optimal batch size\"\"\"\n    nbr_net_req = int(file_size / DB2IXF_BUFFER_SIZE_CLOUD_PROVIDER)\n    if nbr_net_req == 0:\n        nbr_net_req = 1\n    return int(nbr_net_req * DB2IXF_DEFAULT_BATCH_SIZE * DB2IXF_RISK_FACTOR)\n</code></pre>"},{"location":"markdown/code/helpers/#db2ixf.helpers.get_pyarrow_schema","title":"<code>get_pyarrow_schema(cols)</code>","text":"<p>Creates a pyarrow schema of the columns extracted from IXF file.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>List[OrderedDict]</code> <p>List of column descriptors extracted from IXF file.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Pyarrow Schema extracted from columns description.</p> Source code in <code>src/db2ixf/helpers.py</code> Python<pre><code>def get_pyarrow_schema(cols: List[OrderedDict]) -&gt; Schema:\n    \"\"\"\n    Creates a pyarrow schema of the columns extracted from IXF file.\n\n    Parameters\n    ----------\n    cols : List[OrderedDict]\n        List of column descriptors extracted from IXF file.\n\n    Returns\n    -------\n    Schema\n        Pyarrow Schema extracted from columns description.\n    \"\"\"\n\n    mapper = {\n        \"DATE\": date32(),\n        \"TIME\": time64(\"ns\"),\n        \"TIMESTAMP\": timestamp(\"ns\", DB2IXF_TIME_ZONE),\n        \"BLOB\": large_binary(),\n        \"CLOB\": large_string(),\n        \"VARCHAR\": string(),\n        \"CHAR\": string(),\n        \"LONGVARCHAR\": string(),\n        \"VARGRAPHIC\": string(),\n        \"FLOATING POINT\": float64(),\n        \"DECIMAL\": decimal128(19),\n        \"BIGINT\": int64(),\n        \"INTEGER\": int32(),\n        \"SMALLINT\": int16(),\n        \"BINARY\": binary(),\n    }\n\n    # todo: use the code page from the header instead of utf-8  # noqa\n    _schema = []\n    for c in cols:\n        cname = c[\"IXFCNAME\"].decode(\"utf-8\").strip()\n        # cdesc = c[\"IXFCDESC\"].decode(\"utf-8\").strip()\n        cnull = c[\"IXFCNULL\"].decode(\"utf-8\").strip()\n        ctype = int(c[\"IXFCTYPE\"])\n\n        # Update dtype for datatypes\n        dtype = mapper[IXF_DTYPES[ctype]]\n        if ctype == 912:\n            length = int(c[\"IXFCLENG\"])\n            dtype = binary(length)\n\n        if ctype == 480:\n            length = int(c[\"IXFCLENG\"])\n            dtype = float32() if length == 4 else dtype\n\n        if ctype == 484:\n            precision = int(c[\"IXFCLENG\"][0:3])\n            scale = int(c[\"IXFCLENG\"][3:5])\n            if precision &lt;= 38:\n                dtype = decimal128(precision, scale)\n            else:\n                dtype = decimal256(precision, scale)\n\n        if ctype == 392:\n            fsp = int(c[\"IXFCLENG\"])\n            if fsp == 0:\n                dtype = timestamp(\"s\", DB2IXF_TIME_ZONE)\n            elif 0 &lt; fsp &lt;= 3:\n                dtype = timestamp(\"ms\", DB2IXF_TIME_ZONE)\n            elif 3 &lt; fsp &lt;= 6:\n                dtype = timestamp(\"us\", DB2IXF_TIME_ZONE)\n            elif 6 &lt; fsp &lt;= 12:\n                dtype = timestamp(\"ns\", DB2IXF_TIME_ZONE)\n            else:\n                msg = f\"Invalid time precision for {cname}, expected &lt; 12\"\n                raise NotValidDataPrecisionException(msg)\n\n        # See if the col is nullable or not\n        if cnull.lower() not in [\"y\", \"n\"]:\n            cnull = \"Y\"\n        cnullable = False if cnull.lower() == \"n\" else True\n\n        _field = field(\n            cname,\n            dtype,\n            nullable=cnullable,\n            # metadata={cname: cdesc}\n        )\n\n        _schema.append(_field)\n\n    return schema(_schema)\n</code></pre>"},{"location":"markdown/code/helpers/#db2ixf.helpers.get_ccsid_from_column","title":"<code>get_ccsid_from_column(column)</code>","text":"<p>Get the coded character set identifiers for single and double bytes data type. Which means the code page for singular/double byte data type.</p> Source code in <code>src/db2ixf/helpers.py</code> Python<pre><code>def get_ccsid_from_column(column: OrderedDict) -&gt; Tuple[int, int]:\n    \"\"\"\n    Get the coded character set identifiers for single and double bytes\n    data type. Which means the code page for singular/double byte data type.\n    \"\"\"\n    sbcp = str(column[\"IXFCSBCP\"], \"utf-8\").strip()\n    dbcp = str(column[\"IXFCDBCP\"], \"utf-8\").strip()\n\n    sbcp = int(sbcp) if sbcp else 0\n    dbcp = int(dbcp) if dbcp else 0\n\n    return sbcp, dbcp\n</code></pre>"},{"location":"markdown/code/helpers/#db2ixf.helpers.deltalake_fix_ns_timestamps","title":"<code>deltalake_fix_ns_timestamps(pyarrow_schema)</code>","text":"<p>Fix issue with timestamps in deltalake.</p> <p>Deltalake has issue with timestamps in nanoseconds and it does not yet support it, so this function changes the pyarrow timestamp datatype in nanoseconds to microseconds. pyarrow timestamp datatype in microseconds is supported.</p> <p>Parameters:</p> Name Type Description Default <code>pyarrow_schema</code> <code>Schema</code> <p>Pyarrow schema</p> required <p>Returns:</p> Name Type Description <code>Schema</code> <code>Schema</code> <p>Pyarrow schema with fix</p> Source code in <code>src/db2ixf/helpers.py</code> Python<pre><code>def deltalake_fix_ns_timestamps(pyarrow_schema: Schema) -&gt; Schema:\n    \"\"\"Fix issue with timestamps in deltalake.\n\n    Deltalake has issue with timestamps in nanoseconds and it does not yet\n    support it, so this function changes the pyarrow timestamp datatype\n    in nanoseconds to microseconds. pyarrow timestamp datatype in microseconds\n    is supported.\n\n    Parameters\n    ----------\n    pyarrow_schema : Schema\n        Pyarrow schema\n\n    Returns\n    -------\n    Schema:\n        Pyarrow schema with fix\n    \"\"\"\n    for i, f in enumerate(pyarrow_schema):\n        if f.type == timestamp(\"ns\", DB2IXF_TIME_ZONE):\n            new_field = field(\n                f.name,\n                timestamp(\"us\", DB2IXF_TIME_ZONE),\n                nullable=f.nullable,\n                # metadata=f.metadata\n            )\n            pyarrow_schema = pyarrow_schema.set(i, new_field)\n    return pyarrow_schema\n</code></pre>"},{"location":"markdown/code/helpers/#db2ixf.helpers.deltalake_fix_time","title":"<code>deltalake_fix_time(pyarrow_schema)</code>","text":"<p>Fix issue with time in deltalake.</p> <p>Deltalake does not support time datatype so we will try to use string to temporary fix the issue. Pyarrow schema has time64 and time32 datatypes but it is complicated for now to cast them to timestamp because the later is supported by deltalake. For this later reason, this function will use pyarrow string datatype to replace pyarrow time datatypes until casting pyarrow time datatype as a datetime is possible in deltalake or support of pyarrow time datatype in deltalake is added.</p> <p>Parameters:</p> Name Type Description Default <code>pyarrow_schema</code> <code>Schema</code> <p>Pyarrow schema</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Pyarrow schema with the fix.</p> Source code in <code>src/db2ixf/helpers.py</code> Python<pre><code>def deltalake_fix_time(pyarrow_schema: Schema) -&gt; Schema:\n    \"\"\"Fix issue with time in deltalake.\n\n    Deltalake does not support time datatype so we will try to use string to\n    temporary fix the issue. Pyarrow schema has time64 and time32 datatypes but\n    it is complicated for now to cast them to timestamp because the later is\n    supported by deltalake. For this later reason, this function will use\n    pyarrow string datatype to replace pyarrow time datatypes until casting\n    pyarrow time datatype as a datetime is possible in deltalake or support of\n    pyarrow time datatype in deltalake is added.\n\n    Parameters\n    ----------\n    pyarrow_schema : Schema\n        Pyarrow schema\n\n    Returns\n    -------\n    Schema\n        Pyarrow schema with the fix.\n    \"\"\"\n    time_datatypes = {\n        time64(\"ns\"),\n        time64(\"us\"),\n        time32(\"ms\"),\n        time32(\"s\"),\n    }\n    for i, f in enumerate(pyarrow_schema):\n        if f.type in time_datatypes:\n            new_field = field(\n                f.name,\n                string(),\n                nullable=f.nullable,\n                # metadata=f.metadata\n            )\n            pyarrow_schema = pyarrow_schema.set(i, new_field)\n    return pyarrow_schema\n</code></pre>"},{"location":"markdown/code/helpers/#db2ixf.helpers.apply_schema_fixes","title":"<code>apply_schema_fixes(pyarrow_schema)</code>","text":"<p>Apply all fixes on pyarrow schema to adapt to deltalake.</p> <p>Fixes issues in deltalake support for nanoseconds unit for time and timestamp datatype.</p> <p>Parameters:</p> Name Type Description Default <code>pyarrow_schema</code> <code>Schema</code> <p>Pyarrow schema</p> required <p>Returns:</p> Name Type Description <code>Schema</code> <code>Schema</code> <p>Pyarrow schema with all fixes</p> Source code in <code>src/db2ixf/helpers.py</code> Python<pre><code>def apply_schema_fixes(pyarrow_schema: Schema) -&gt; Schema:\n    \"\"\"Apply all fixes on pyarrow schema to adapt to deltalake.\n\n    Fixes issues in deltalake support for nanoseconds unit for time and\n    timestamp datatype.\n\n    Parameters\n    ----------\n    pyarrow_schema : Schema\n        Pyarrow schema\n\n    Returns\n    -------\n    Schema:\n        Pyarrow schema with all fixes\n    \"\"\"\n    fixes = [deltalake_fix_ns_timestamps, deltalake_fix_time]\n    for fix in fixes:\n        pyarrow_schema = fix(pyarrow_schema)\n    return pyarrow_schema\n</code></pre>"},{"location":"markdown/code/helpers/#db2ixf.helpers.to_pyarrow_record_batch","title":"<code>to_pyarrow_record_batch(batch, pyarrow_schema)</code>","text":"<p>Transforms to pyarrow record batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>DefaultOrderedDict</code> <p>Dictionary of type Dict[str, list]</p> required <code>pyarrow_schema</code> <code>Schema</code> <p>Pyarrow schema</p> required <p>Returns:</p> Type Description <code>RecordBatch</code> <p>Pyarrow record batch</p> Source code in <code>src/db2ixf/helpers.py</code> Python<pre><code>def to_pyarrow_record_batch(\n    batch: dict,\n    pyarrow_schema: Schema\n) -&gt; RecordBatch:\n    \"\"\"Transforms to pyarrow record batch.\n\n    Parameters\n    ----------\n    batch : DefaultOrderedDict\n        Dictionary of type Dict[str, list]\n    pyarrow_schema: Schema\n        Pyarrow schema\n\n    Returns\n    -------\n    RecordBatch\n        Pyarrow record batch\n    \"\"\"\n    _arrays = [array(v) for v in batch.values()]\n    return record_batch(_arrays, schema=pyarrow_schema)\n</code></pre>"},{"location":"markdown/code/helpers/#db2ixf.helpers.decode_cell","title":"<code>decode_cell(cell, cp, cpt='s')</code>","text":"<p>Try to decode the cell using the provided codepage.</p> <p>Parameters:</p> Name Type Description Default <code>cell</code> <code>str</code> <p>Field containing data</p> required <code>cp</code> <code>int</code> <p>IBM code page</p> required <code>cpt</code> <code>Literal['s', 'd']</code> <p>Defaults to <code>s</code> which means single byte and <code>d</code> means double bytes</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Decoded cell</p> Source code in <code>src/db2ixf/helpers.py</code> Python<pre><code>def decode_cell(cell: str, cp: int, cpt: Literal[\"s\", \"d\"] = \"s\"):\n    \"\"\"Try to decode the cell using the provided codepage.\n\n    Parameters\n    ----------\n    cell : str\n        Field containing data\n    cp : int\n        IBM code page\n    cpt : Literal[\"s\", \"d\"]\n        Defaults to `s` which means single byte and `d` means double bytes\n\n    Returns\n    -------\n    str:\n        Decoded cell\n    \"\"\"\n    if cpt not in [\"s\", \"d\"]:\n        raise ValueError(\"Either `s` for single bytes or `d` for double bytes\")\n\n    try:\n        return cell.decode(f\"cp{cp}\")\n    except UnicodeDecodeError:\n        logger.debug(\"Trying cp437 encoding\")\n        try:\n            return cell.decode(\"cp437\")\n        except UnicodeDecodeError:\n            try:\n                logger.debug(\"Trying to detect the encoding\")\n                _encoding = chardet.detect(cell, True)[\"encoding\"]\n                return cell.decode(_encoding)\n            except UnicodeDecodeError as err:\n                logger.debug(f\"Detected encoding fails: {err}\")\n                try:\n                    if cpt == \"s\":\n                        logger.debug(\"Trying utf-8 encoding\")\n                        return cell.decode(\"utf-8\")\n                    else:\n                        try:\n                            logger.debug(\"Trying utf-16 encoding\")\n                            return cell.decode(\"utf-16\")\n                        except UnicodeDecodeError:\n                            logger.debug(\"Trying utf-32 encoding\")\n                            return cell.decode(\"utf-32\")\n                except UnicodeDecodeError:\n                    logger.debug(\n                        \"Alert: eventual data loss, please provide encoding !\"\n                    )\n                    return cell.decode(f\"cp{cp}\", errors=\"ignore\")\n</code></pre>"}]}